{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------\n",
    "### Course Project PCQI: Time-Dependent processes with Neural Quantum States\n",
    "------------------------------------------------------------\n",
    "This notebook is part of the course project for PCQI 2022.\n",
    "By Pim Veefkind (XXXXXXX) & Thomas Rothe (1930443)\n",
    "\n",
    "Set-up of basic NQS framework & implementation of time evolution + evaluation of expectations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------\n",
    "#### Basic set-up of NQS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1./(np.exp(X)+1)\n",
    "\n",
    "def LnRMBWavefunction(W,a,c,V):\n",
    "    #\n",
    "    # Golden rule of numerics: avoid exponentials.\n",
    "    # Use ln's instead.\n",
    "    #\n",
    "    Wsummed = 0\n",
    "    LnPreFactor = 0\n",
    "    L = V.shape[0]\n",
    "    for s in range(L):\n",
    "        Wsummed = Wsummed + W[:,s]*V[s]\n",
    "        LnPreFactor = LnPreFactor - a[s]*V[s]\n",
    "    \n",
    "    # Difference between bits 0 and 1 and spins -1 and 1\n",
    "    LnPrePreFactor = np.sum(a)/2 + np.sum(c)/2+np.sum(W)/4\n",
    "    AngleFactor = np.prod(1+np.exp(-c - Wsummed))\n",
    "    LnPsiRMB = LnPrePreFactor + LnPreFactor + np.log(AngleFactor)\n",
    "    return LnPsiRMB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC (Metropolis) Simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MetropolisCycle(W,a,c,Vt):\n",
    "    rejectvalue = 0   \n",
    "    LnPsiOld = LnRMBWavefunction(W,a,c,Vt)\n",
    "    #\n",
    "    # Flip a random spin\n",
    "    # \n",
    "    L = Vt.shape[0] \n",
    "    site = np.random.randint(L)\n",
    "    Vt[site] = - Vt[site] +1\n",
    "    LnPsiNew = LnRMBWavefunction(W,a,c,Vt)\n",
    "    #\n",
    "    acceptanceratio = np.exp(np.real(np.conj(LnPsiNew)+LnPsiNew-np.conj(LnPsiOld)-LnPsiOld))\n",
    "    #if acceptanceratio #MISSING INEQUALITY SIGN# 1:\n",
    "    if acceptanceratio >= 1:\n",
    "        return Vt,rejectvalue\n",
    "    else:\n",
    "        p = np.random.rand()\n",
    "        #if p #MISSING INEQUALITY SIGN# acceptanceratio:\n",
    "        if p >= acceptanceratio:\n",
    "            rejectvalue = 1\n",
    "            Vt[site] = - Vt[site] + 1\n",
    "            \n",
    "        return Vt,rejectvalue\n",
    "\n",
    "def MetropolisSamp(W,a,c,V,k):\n",
    "    #\n",
    "    # Burn-in to get rid of initial condition dependence\n",
    "    #\n",
    "    rejections = 0\n",
    "    rejectvalue = 0\n",
    "    for z in range(10000):\n",
    "        Vt = V\n",
    "        V,rejectvalue = MetropolisCycle(W,a,c,Vt)\n",
    "        rejections = rejections + rejectvalue\n",
    "    \n",
    "    print('Percentage Rejections in Burn-in: %.2f %%' %(rejections/100))\n",
    "    #\n",
    "    #\n",
    "    # We collect the full sequence of spin configurations V\n",
    "    # Together they form a efficient short representation of the full distribution\n",
    "    # \n",
    "    rejections = 0\n",
    "    rejectvalue = 0\n",
    "    Vensemble = np.copy(V)\n",
    "    L = np.shape(V)[0]\n",
    "    for z in range(k):\n",
    "        # initiate sweep, i.e. cycle over # visible spins between appending\n",
    "        for zz in range(L):\n",
    "            V,rejectvalue = MetropolisCycle(W,a,c,V)\n",
    "        Vensemble = np.append(Vensemble,V)\n",
    "        rejections = rejections + rejectvalue\n",
    "    \n",
    "    prctrej = 100*rejections/k\n",
    "    #print('Percentage Rejections in Ensemble: %.1f %% (%i/%i)' %(prctrej,rejections,k))\n",
    "    Vensemble_reshape = Vensemble.reshape((k+1,L))\n",
    "    # print(Vensemble_reshape)\n",
    "    return Vensemble_reshape, prctrej "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization of (time-independent) cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elocal(J,B,W,a,c,V):\n",
    "    #\n",
    "    # Computing the wavefunction for state V\n",
    "    #\n",
    "    L = V.shape[0]\n",
    "    LnPsi = LnRMBWavefunction(W,a,c,V)\n",
    "    LnPsiBar = np.conj(LnPsi)\n",
    "    #\n",
    "    # Computing the energy for state V\n",
    "    # First the Ising term\n",
    "    #\n",
    "    Vshift = np.array([V[(i+1)%L] for i in range(L)])\n",
    "    One = np.ones(L)\n",
    "    ElocalJ = -J*(np.sum((2*V-One)*(2*Vshift-One)))\n",
    "    #\n",
    "    # Next the magnetic term -B\\sum_i \\sigma^x_i\n",
    "    # Because this is not diagonal on the\n",
    "    # states, we compute \n",
    "    # <V|EB|Psi> instead\n",
    "    # The action of Sigma^x_i is\n",
    "    # to flip the spin on site i:\n",
    "    # i.e. map V[i] to -V[i]+1\n",
    "    #\n",
    "    EBlocalPsi = 0\n",
    "    for i in range(L):\n",
    "        V[i] = -V[i]+1\n",
    "        EBlocalPsi = EBlocalPsi - B*np.exp(LnRMBWavefunction(W,a,c,V)-LnPsi) #Compare flipped with unflipped (sigma_x applied)\n",
    "        V[i] = -V[i]+1\n",
    "    \n",
    "    ElocalPsi = ElocalJ + EBlocalPsi\n",
    "    \n",
    "    return ElocalPsi\n",
    "\n",
    "    \n",
    "\n",
    "def WeightUpdateSmoothed(J,B,W,a,c,Vensemble,lrate,ep):   \n",
    "    # \n",
    "    # Now we will see the advantage of the Monte Carlo sampling.\n",
    "    # Instead of summing over all spin configurations, \n",
    "    # we sum only over the ones generated by the MCMC Metropolis/Gibbs routine\n",
    "    # to compute expectation values:\n",
    "    # <Psi|Operator|Psi> = \\sum_{all S,S'} <Psi|S><S|Operator|S'><S'|Psi>\n",
    "    # is approximated by\n",
    "    # <Psi|Operator|Psi> \\simeq \\sum_{Gibbs S,S'} <Psi|S><S|Operator|S'><S'|Psi>\n",
    "    # For L large dim(S)=2^L, whereas we only need a finite number of Gibbs samples\n",
    "    # So this will help greatly at large L\n",
    "    #\n",
    "    LenEnsemb = Vensemble.shape[0]\n",
    "    L = Vensemble.shape[1]\n",
    "    H = c.shape[0]\n",
    "    #\n",
    "    # Initializing for ensemble Exp(ectation)Val(ue)\n",
    "    #\n",
    "    LnNormPsi = 0\n",
    "    EExpVal = 0\n",
    "    ElocalExpVal = 0\n",
    "    ElocalVExpVal = 0\n",
    "    ElocalHExpVal = 0\n",
    "    ElocalWExpVal = 0\n",
    "    VExpVal = 0\n",
    "    HExpVal = 0\n",
    "    WExpVal = 0\n",
    "    agradientEExpVal = 0\n",
    "    cgradientEExpVal = 0\n",
    "    WgradientEExpVal = 0\n",
    "    derivsExpVal = 0\n",
    "    moment2ExpVal = 0\n",
    "    for l in range(LenEnsemb):\n",
    "        V = Vensemble[l]\n",
    "        #\n",
    "        # V now labels a particular state\n",
    "        #\n",
    "        # Computing the energy for state V\n",
    "        #\n",
    "        ElocalPsi = Elocal(J,B,W,a,c,V)\n",
    "        #\n",
    "        # Next we compute \n",
    "        # <V|EV|V> = Elocal*V\n",
    "        # <V|EH|V> = <Esigmoid(WV+c)> =Elocal*\n",
    "        # <V|EHV|V> = <EVsigmoid(WV+c)>\n",
    "        #\n",
    "        ElocalVPsi = ElocalPsi*V \n",
    "        ElocalHPsi = ElocalPsi*sigmoid(c + np.matmul(W,V))  #sigmoid = current h vector\n",
    "        ElocalWPsi = ElocalPsi*np.outer(sigmoid(c + np.matmul(W,V)),V)\n",
    "        # \n",
    "        # Next we compute \n",
    "        # <V>\n",
    "        # <H>\n",
    "        # <HV>\n",
    "        #\n",
    "        derivs = np.concatenate((V,np.real(sigmoid(c+np.matmul(W,V))),np.real(np.outer(sigmoid(c+np.matmul(W,V)),V)).reshape(L*H)))\n",
    "        #\n",
    "        # Matrix of conj.derivs \\times derivs\n",
    "        #\n",
    "        moment2 = np.outer(np.conj(derivs),derivs)\n",
    "        #\n",
    "        # Computing ensemble averages (uniform distrib. over all sampled configs)\n",
    "        #\n",
    "        ElocalExpVal = ElocalExpVal + ElocalPsi/LenEnsemb\n",
    "        ElocalVExpVal = ElocalVExpVal + np.real(ElocalVPsi)/(LenEnsemb)\n",
    "        ElocalHExpVal = ElocalHExpVal + np.real(ElocalHPsi)/(LenEnsemb)\n",
    "        ElocalWExpVal = ElocalWExpVal + np.real(ElocalWPsi)/(LenEnsemb)\n",
    "        derivsExpVal = derivsExpVal + derivs/LenEnsemb\n",
    "        moment2ExpVal = moment2ExpVal + moment2/LenEnsemb\n",
    "        #\n",
    "        \n",
    "    # Statistical local gradients, ignoring the quantum mechanical term\n",
    "    #\n",
    "    VExpVal = derivsExpVal[:L]\n",
    "    HExpVal = derivsExpVal[L:L+H]\n",
    "    WExpVal = derivsExpVal[L+H:].reshape(H,L)\n",
    "    agradientEStat = - ElocalVExpVal + ElocalExpVal*VExpVal\n",
    "    cgradientEStat = - ElocalHExpVal + ElocalExpVal*HExpVal\n",
    "    WgradientEStat = - ElocalWExpVal + ElocalExpVal*WExpVal\n",
    "    #\n",
    "    # Computing metric on Probability space\n",
    "    #\n",
    "    #   - Cartesian metric as default\n",
    "    #\n",
    "    S_kkCartesian = np.diag(np.ones(L*H+L+H))\n",
    "    #\n",
    "    #   - Sorella version\n",
    "    #\n",
    "    S_kkSorella = moment2ExpVal - np.outer(np.conj(derivsExpVal),derivsExpVal)\n",
    "    #\n",
    "    #   - Regulator necessary to ensure inverse exists\n",
    "    #\n",
    "    lreg = np.max(np.array([100*(0.9)**ep,0.01]))  \n",
    "    S_kkSorellaReg = S_kkSorella + lreg * np.diag(np.diag(S_kkCartesian))\n",
    "    #\n",
    "    #S_kk = S_kkCartesian\n",
    "    S_kk = S_kkSorellaReg #Sorella = use variance in parameters/their derivates to adjust learning rate individually (per parameter type, per parameter)!\n",
    "    #print(\"Average eigenvalue Sorella matrix:\", np.trace(S_kk)/(L*H+L+H))\n",
    "    #print(S_kk[L+H:,L+H:])\n",
    "    #\n",
    "    agrad = np.copy(agradientEStat)\n",
    "    cgrad = np.copy(cgradientEStat)\n",
    "    Wgrad = np.copy(WgradientEStat)\n",
    "    #\n",
    "    # Print out average length-squared of gradients as diagnostic\n",
    "    # (finding good initial guess of model parameters manually)\n",
    "    #\n",
    "    GradAAbsSq = np.real(np.inner(np.conj(agrad),agrad))/L\n",
    "    GradCAbsSq = np.real(np.inner(np.conj(cgrad),cgrad))/H\n",
    "    GradWAbsSq = np.real(np.sum(np.conj(Wgrad)*Wgrad))/(L*H)\n",
    "    print('\\rGradient absval-squared: a: %.4f, c: %.4f, W: %.4f. ' %(GradAAbsSq,GradCAbsSq,GradWAbsSq), end='')\n",
    "    #\n",
    "    #\n",
    "    Wgradtemp = Wgrad.reshape(L*H)\n",
    "    paras = np.concatenate((a,c,W.reshape(L*H)))\n",
    "    gradE = np.conj(np.concatenate((agrad,cgrad,Wgradtemp)))\n",
    "    #\n",
    "    deltaparas = lrate * np.einsum('ij,j->i',np.linalg.inv(S_kk),gradE) #Learning rate in metric x gradient\n",
    "    paras = paras - deltaparas #Update parameters (collectively in one big array)\n",
    "    print('Average weight update size:', np.average(deltaparas))\n",
    "    #\n",
    "    #\n",
    "    a = paras[:L]\n",
    "    c = paras[L:L+H]\n",
    "    W = paras[L+H:].reshape(H,L)\n",
    "    #\n",
    "    #print('Local Energy: ', ElocalExpVal)\n",
    "    #\n",
    "    return W,a,c,ElocalExpVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main simulation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NQSRBM(J,B,Nv,Nh,kContrastDiv,lrate,epochs):\n",
    "    # Service message\n",
    "    print(\"\"\"\\\n",
    "        Neural Quantum State of the transverse field Ising model:\n",
    "        Ising model parameters J, B: %f, %f\n",
    "        Number of visible spins: %i\n",
    "        Number of hidden spins: %i\n",
    "        Monte Carlo sequence size: %i\n",
    "        Learning Rate: %f\n",
    "        Epochs: %i\n",
    "        \"\"\" %(J,B,Nv,Nh,kContrastDiv,lrate,epochs))\n",
    "    #\n",
    "    # Initializing weights with real values between -1 and 1\n",
    "    # The system is VERY sensitive to initial conditions. \n",
    "    # E.g. it will not converge if all weights are negative.\n",
    "    #\n",
    "    W0 = (0.2)*(2*np.random.rand(Nh,Nv)-1. +np.random.rand(Nh,Nv)*1j)\n",
    "    a0 = (0.1)*(2*np.random.rand(Nv)-1. + np.random.rand(Nv)*1j)\n",
    "    c0 = (0.1)*(2*np.random.rand(Nh)-1. + np.random.rand(Nh)*1j)\n",
    "    #W0 = np.random.normal(size=(Nh,Nv))/1e4\n",
    "    #a0 = np.random.normal(size=(Nv))/10\n",
    "    #c0 = np.random.normal(size=(Nh))/1e4\n",
    "    W0 = np.real(W0)\n",
    "    a0 = np.real(a0)\n",
    "    c0 = np.real(c0)\n",
    "    #\n",
    "    # Initialing visible spins with either 0 or 1\n",
    "    #\n",
    "    V0 = np.random.choice([0,1],Nv)\n",
    "    #\n",
    "    Magnetization = np.sum(V0)-Nv/2\n",
    "    #while Magnetization > 0:\n",
    "    #    site = np.random.randint(Nv)\n",
    "    #    print('Flip-site', site)\n",
    "    #    if V0[site] > 0 :\n",
    "    #        V0[site] = - V0[site] + 1 \n",
    "    #    Magnetization = np.sum(V0)-Nv/2\n",
    "    #while Magnetization < 0:\n",
    "    #    site = np.random.randint(Nv)\n",
    "    #    print('Flip-site', site)\n",
    "    #    if V0[site] == 0:\n",
    "    #        V0[site] = - V0[site] + 1 \n",
    "    #    Magnetization = np.sum(V0)-Nv/2\n",
    "    #Magnetization = np.sum(V0)-Nv/2\n",
    "    print('Magnetization Initial state: ', Magnetization)\n",
    "    #\n",
    "    # Learning/Variational Minimization cycle\n",
    "    #\n",
    "    V = np.copy(V0)\n",
    "    W = np.copy(W0)\n",
    "    a = np.copy(a0)\n",
    "    c = np.copy(c0)\n",
    "    #\n",
    "    # The transverse field Ising model happens to\n",
    "    # be exactly solvable through other means.\n",
    "    # We secretly know the exact GS energy:\n",
    "    #\n",
    "    g = B/J\n",
    "    FreeFermionModes = np.sqrt(1 + g**2-2*g*np.cos(2*np.pi*np.arange(Nv)/Nv)) \n",
    "    EexactPerSite = -J*np.sum(FreeFermionModes)/Nv #Number of modes on each site * energy of occupation = interaction energy\n",
    "    #\n",
    "    # Variable Initialization for plotting results\n",
    "    #\n",
    "    Convergence = np.array([[1,1]])\n",
    "    Percentage = np.array([0])\n",
    "    prct = 0\n",
    "    #\n",
    "    for ep in range(epochs):\n",
    "        #\n",
    "        Vensemble,prct = MetropolisSamp(W,a,c,V,kContrastDiv) #Get  representative samples\n",
    "        W,a,c,EExpVal = WeightUpdateSmoothed(J,B,W,a,c,Vensemble,lrate,ep) #Update paramters by fixed paramter gradients on ensemble\n",
    "        EVarPerSite = np.real(EExpVal)/Nv\n",
    "        Convergence = np.append(Convergence,np.array([[ep,EVarPerSite]]),axis=0)\n",
    "        Percentage = np.append(Percentage,np.array([prct]),axis=0)\n",
    "        #lrate = lrate * 0.95 \n",
    "        print('\\rEpoch %i/%i: Variational Energy: %f, Exact Energy: %f ' %(ep+1,epochs,EVarPerSite,EexactPerSite), end='')\n",
    "        if not np.abs(EVarPerSite) < 10e6:\n",
    "            print('\\nNumerical Runaway: discontinuing...')\n",
    "            break\n",
    "        #print('Weights updated: Started learning epoch %i out of %i\\n' %(ep+1,epochs))\n",
    "    \n",
    "    WRBM = np.copy(W)\n",
    "    aRBM = np.copy(a)\n",
    "    cRBM = np.copy(c)\n",
    "    sampler = 'MetroSmoothed'\n",
    "    filename = f'NQSdata_J{J:01}_h{B:01}_{sampler}_Cycles{kContrastDiv}_Epochs{epochs}.pickle'\n",
    "    print('\\nFile = ', filename)\n",
    "    results = (Convergence, Percentage, aRBM, cRBM, WRBM, EexactPerSite)\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(results,f)\n",
    "        \n",
    "    return results     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_time_independent_convergence(Convergence,Percentage, EexactPerSite):\n",
    "    Eexc = EexactPerSite*np.ones(Convergence.shape[0]-1)\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(Convergence[1:,0],Convergence[1:,1], label=\"Simulated energy per site\")\n",
    "    ax.plot(Convergence[1:,0],Eexc, label=\"Exact energy per site\")\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(Convergence[1:,0],Percentage[1:],color='red',linestyle=':', label=\"Rejection rate\")\n",
    "    ax2.set_ylim(0,100)\n",
    "    \n",
    "    ax.set_title('Convergence')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(r'${E_{loc}}/{L}$')\n",
    "    ax2.set_ylabel(\"Rejection rate\")\n",
    "    ax.legend()\n",
    "    fig;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for running a time-independent simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NQSrun = NQSRBM(1.,0.5,10,40,6000,0.4,52)\n",
    "Convergence,Percentage, aRBM, cRBM, WRBM, EexactPerSite = NQSrun\n",
    "#\n",
    "# Displaying analytics\n",
    "#\n",
    "plot_time_independent_convergence(Convergence, Percentage, EexactPerSite)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac5fa55e59d7907f2234b868d4eba807892b77462284fb15ceb9b28ad8dae652"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
