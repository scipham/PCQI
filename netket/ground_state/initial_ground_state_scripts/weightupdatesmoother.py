import numpy as np

from .sigmoid import sigmoid
from .elocal import Elocal

def WeightUpdateSmoothed(J,B,W,a,c,Vensemble,lrate,ep):   
    # 
    # Now we will see the advantage of the Monte Carlo sampling.
    # Instead of summing over all spin configurations, 
    # we sum only over the ones generated by the MCMC Metropolis/Gibbs routine
    # to compute expectation values:
    # <Psi|Operator|Psi> = \sum_{all S,S'} <Psi|S><S|Operator|S'><S'|Psi>
    # is approximated by
    # <Psi|Operator|Psi> \simeq \sum_{Gibbs S,S'} <Psi|S><S|Operator|S'><S'|Psi>
    # For L large dim(S)=2^L, whereas we only need a finite number of Gibbs samples
    # So this will help greatly at large L
    #
    LenEnsemb = Vensemble.shape[0]
    L = Vensemble.shape[1]
    H = c.shape[0]
    #
    # Initializing for ensemble Exp(ectation)Val(ue)
    #
    LnNormPsi = 0
    EExpVal = 0
    ElocalExpVal = 0
    ElocalVExpVal = 0
    ElocalHExpVal = 0
    ElocalWExpVal = 0
    VExpVal = 0
    HExpVal = 0
    WExpVal = 0
    agradientEExpVal = 0
    cgradientEExpVal = 0
    WgradientEExpVal = 0
    derivsExpVal = 0
    moment2ExpVal = 0
    for l in range(LenEnsemb):
        V = Vensemble[l]
        #
        # V now labels a particular state
        #
        # Computing the energy for state V
        #
        ElocalPsi = Elocal(J,B,W,a,c,V)
        #
        # Next we compute 
        # <V|EV|V> = Elocal*V
        # <V|EH|V> = <Esigmoid(WV+c)> =Elocal*
        # <V|EHV|V> = <EVsigmoid(WV+c)>
        #
        ElocalVPsi = ElocalPsi*V 
        ElocalHPsi = ElocalPsi*sigmoid(c + np.matmul(W,V))  #sigmoid = current h vector
        ElocalWPsi = ElocalPsi*np.outer(sigmoid(c + np.matmul(W,V)),V)
        # 
        # Next we compute 
        # <V>
        # <H>
        # <HV>
        #
        derivs = np.concatenate((V,np.real(sigmoid(c+np.matmul(W,V))),np.real(np.outer(sigmoid(c+np.matmul(W,V)),V)).reshape(L*H)))
        #
        # Matrix of conj.derivs \times derivs
        #
        moment2 = np.outer(np.conj(derivs),derivs)
        #
        # Computing ensemble averages (uniform distrib. over all sampled configs)
        #
        ElocalExpVal = ElocalExpVal + ElocalPsi/LenEnsemb
        ElocalVExpVal = ElocalVExpVal + np.real(ElocalVPsi)/(LenEnsemb)
        ElocalHExpVal = ElocalHExpVal + np.real(ElocalHPsi)/(LenEnsemb)
        ElocalWExpVal = ElocalWExpVal + np.real(ElocalWPsi)/(LenEnsemb)
        derivsExpVal = derivsExpVal + derivs/LenEnsemb
        moment2ExpVal = moment2ExpVal + moment2/LenEnsemb
        #
        
    # Statistical local gradients, ignoring the quantum mechanical term
    #
    VExpVal = derivsExpVal[:L]
    HExpVal = derivsExpVal[L:L+H]
    WExpVal = derivsExpVal[L+H:].reshape(H,L)
    agradientEStat = - ElocalVExpVal + ElocalExpVal*VExpVal
    cgradientEStat = - ElocalHExpVal + ElocalExpVal*HExpVal
    WgradientEStat = - ElocalWExpVal + ElocalExpVal*WExpVal
    #
    # Computing metric on Probability space
    #
    #   - Cartesian metric as default
    #
    S_kkCartesian = np.diag(np.ones(L*H+L+H))
    #
    #   - Sorella version
    #
    S_kkSorella = moment2ExpVal - np.outer(np.conj(derivsExpVal),derivsExpVal)
    #
    #   - Regulator necessary to ensure inverse exists
    #
    lreg = np.max(np.array([100*(0.9)**ep,0.01]))  
    S_kkSorellaReg = S_kkSorella + lreg * np.diag(np.diag(S_kkCartesian))
    #
    #S_kk = S_kkCartesian
    S_kk = S_kkSorellaReg #Sorella = use variance in parameters/their derivates to adjust learning rate individually (per parameter type, per parameter)!
    #print("Average eigenvalue Sorella matrix:", np.trace(S_kk)/(L*H+L+H))
    #print(S_kk[L+H:,L+H:])
    #
    agrad = np.copy(agradientEStat)
    cgrad = np.copy(cgradientEStat)
    Wgrad = np.copy(WgradientEStat)
    #
    # Print out average length-squared of gradients as diagnostic
    # (finding good initial guess of model parameters manually)
    #
    GradAAbsSq = np.real(np.inner(np.conj(agrad),agrad))/L
    GradCAbsSq = np.real(np.inner(np.conj(cgrad),cgrad))/H
    GradWAbsSq = np.real(np.sum(np.conj(Wgrad)*Wgrad))/(L*H)
    print('\rGradient absval-squared: a: %.4f, c: %.4f, W: %.4f. ' %(GradAAbsSq,GradCAbsSq,GradWAbsSq), end='')
    #
    #
    Wgradtemp = Wgrad.reshape(L*H)
    paras = np.concatenate((a,c,W.reshape(L*H)))
    gradE = np.conj(np.concatenate((agrad,cgrad,Wgradtemp)))
    #
    deltaparas = lrate * np.einsum('ij,j->i',np.linalg.inv(S_kk),gradE) #Learning rate in metric x gradient
    paras = paras - deltaparas #Update parameters (collectively in one big array)
    print('Average weight update size:', np.average(deltaparas))
    #
    #
    a = paras[:L]
    c = paras[L:L+H]
    W = paras[L+H:].reshape(H,L)
    #
    #print('Local Energy: ', ElocalExpVal)
    #
    return W,a,c,ElocalExpVal